{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c12e5c67",
   "metadata": {},
   "source": [
    "# Policy Evaluation of User Reviews\n",
    "\n",
    "This notebook demonstrates the workflow for evaluating user-submitted reviews against a set of trustworthiness policies, including spam detection, relevance, and credibility. We start by loading annotated test data, then process reviews in batches using a policy evaluation model. The predictions are saved, analyzed, and compared against ground truth labels. Finally, we compute evaluation metrics such as F1 scores and display the results in a structured table for easy inspection.\n",
    "\n",
    "This pipeline provides a transparent and reproducible approach to assessing review quality and identifying policy violations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ef0679",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../src')\n",
    "\n",
    "from policies.evaluators import PolicyEvaluator\n",
    "from policies.review_selector import select_violated_reviews\n",
    "from objects import Review, Business, OutputData\n",
    "from llm import Model\n",
    "import json, csv\n",
    "from utils import convert_dict_to_review\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c41b80f8",
   "metadata": {},
   "source": [
    "### Loading Annotated Review Data\n",
    "\n",
    "The first step in our pipeline is to load the annotated test dataset, which contains metadata and ground truth labels for each review. We read the CSV file and store all rows in a list for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52919935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "INPUT_FILE = \"../data/processed/test_set_annotated.csv\"\n",
    "OUTPUT_FILE = \"../data/output/test_set_results.jsonl\"\n",
    "\n",
    "with open(INPUT_FILE, newline=\"\") as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    rows = list(reader) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60581ac8",
   "metadata": {},
   "source": [
    "### Batch Evaluation of Reviews\n",
    "\n",
    "After loading the annotated dataset, we process the reviews in batches to efficiently evaluate them with our PolicyEvaluator model. Each batch is converted into Review objects before being passed to the evaluator. The model returns predictions for each review, which are then paired with their ground truth labels and saved to a JSONL file for downstream analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868a3ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator= PolicyEvaluator()\n",
    "batch_size = 50\n",
    "with open(OUTPUT_FILE, \"w\") as outfile: \n",
    "    for batch_start in range(0, len(rows), batch_size):\n",
    "    \n",
    "        batch_end = min(batch_start + batch_size, len(rows))\n",
    "        batch_rows = rows[batch_start:batch_end]\n",
    "        \n",
    "        # Convert batch to Review objects\n",
    "        review_objects = [convert_dict_to_review(batch_start + i, review) \n",
    "                         for i, review in enumerate(batch_rows)]\n",
    "        \n",
    "        # Process entire batch\n",
    "        results = evaluator.evaluate_batch(review_objects)\n",
    "        print(results)\n",
    "\n",
    "        # Write batch results\n",
    "        for result in results:\n",
    "            output = {\n",
    "                \"id\": getattr(result, \"id\", result.id),\n",
    "                \"prediction\": result.evaluation,\n",
    "                \"truth\": result.truth\n",
    "            }\n",
    "            print(output)  # (Optional) print for inspection\n",
    "            outfile.write(json.dumps(output) + \"\\n\")\n",
    "        \n",
    "        print(f\"Processed batch {batch_start//batch_size + 1} \"\n",
    "              f\"(reviews {batch_start+1}-{batch_end})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0fd901",
   "metadata": {},
   "source": [
    "### Evaluating Model Performance\n",
    "\n",
    "To assess the effectiveness of our policy evaluation model, we compute the F1 score and generate a full classification report for each key parameter: credible, relevance, and spam. The predictions from the JSONL output file are compared against the corresponding ground truth labels, with boolean and string values normalized for consistent evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8507591",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "\n",
    "parameters = [\"credible\", \"relevance\", \"spam\"]\n",
    "\n",
    "with open(OUTPUT_FILE) as infile:\n",
    "    lines = infile.readlines()\n",
    "\n",
    "for param in parameters:\n",
    "    y_pred = []\n",
    "    y_true = []\n",
    "    for line in lines:\n",
    "        record = json.loads(line)\n",
    "        pred = record[\"prediction\"][param]\n",
    "        true = record[\"truth\"][param]\n",
    "        # Convert to int (True/False or \"TRUE\"/\"FALSE\")\n",
    "        y_pred.append(int(pred) if isinstance(pred, bool) else int(str(pred).upper() == \"TRUE\"))\n",
    "        y_true.append(int(true) if isinstance(true, bool) else int(str(true).upper() == \"TRUE\"))\n",
    "    print(f\"\\n=== {param.upper()} ===\")\n",
    "    print(f\"F1 score ({param}):\", f1_score(y_true, y_pred))\n",
    "    print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aad4746",
   "metadata": {},
   "source": [
    "### Visualizing Results\n",
    "\n",
    "After processing and evaluating the reviews, we display the results in a structured table using a DataFrame. This allows for easy inspection of predictions, ground truth labels, and any detected policy violations directly within the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2cab4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "violated_reviews = select_violated_reviews(OUTPUT_FILE, INPUT_FILE)\n",
    "df = pd.DataFrame(violated_reviews)\n",
    "display(df)  # This will show a nicely formatted table in the notebook\n",
    "\n",
    "# Optionally, save to CSV as before\n",
    "df.to_csv(\"../data/output/violated_reviews.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eded54af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.6)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
